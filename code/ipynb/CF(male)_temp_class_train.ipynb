{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 17:35:22.851789: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-20 17:35:23.212330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-20 17:35:23.212373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-20 17:35:23.264815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-20 17:35:23.372796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 17:35:24.621990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent factor의 수\n",
    "num_features = 30\n",
    "# iteration 수\n",
    "iterations = 140\n",
    "# learning_rate \n",
    "learning_rate = 1e-1\n",
    "# lambda\n",
    "lambda_ = 1\n",
    "# count_weight\n",
    "count_weight = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/outfit(male)/outfit(male).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# csv 파일을 dataframe으로 변환\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_outfit \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/outfit(male)/outfit(male).csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_weather \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/2022-08-01_to_2024-04-30.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp949\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 필요한 columns만 추출\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ondoset/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ondoset/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/ondoset/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ondoset/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/ondoset/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/outfit(male)/outfit(male).csv'"
     ]
    }
   ],
   "source": [
    "# csv 파일을 dataframe으로 변환\n",
    "df_outfit = pd.read_csv('/home/ksy/code/ondoset_U/ai/data/outfit(male)/outfit(male).csv')\n",
    "df_weather = pd.read_csv('../data/2022-08-01_to_2024-04-30.csv', encoding='cp949')\n",
    "# 필요한 columns만 추출\n",
    "df_outfit = df_outfit[['userId', '상의', '아우터', '하의', '신발', '액세서리', '작성일']].copy()\n",
    "df_temp = df_weather[['일시', '평균기온(°C)']].copy()\n",
    "\n",
    "# '작성일'과 '일시' 열을 datetime 형식으로 변환\n",
    "df_outfit['작성일'] = pd.to_datetime(df_outfit['작성일'], format='%Y년 %m월 %d일')\n",
    "df_temp['일시'] = pd.to_datetime(df_temp['일시'])\n",
    "\n",
    "# 두 dataframe을 날짜를 기준으로 병합\n",
    "df_merged = pd.merge(df_outfit, df_temp, left_on='작성일', right_on='일시').drop('일시', axis=1)\n",
    "\n",
    "'''df_merged'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '상의', '아우터', '하의', '신발', '엑세서리' 열의 결측값을 '~ 없음'으로 대체\n",
    "columns = ['상의', '아우터', '하의', '신발', '액세서리']\n",
    "df_notnull = df_merged.copy()\n",
    "for column in columns:\n",
    "    df_notnull[column] = df_merged[column].fillna(column + ' 없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_notnull[df_notnull['아우터'].str.contains('재킷 2')]\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_notnull[df_notnull['아우터'].str.contains('재킷 2')]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_dup[df_dup['아우터'].str.contains('니트')]\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_dup[df_dup['아우터'].str.contains('니트')]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2가 붙은 단어를 두 번 반복하는 함수\n",
    "def duplicate_word(text):\n",
    "    words = text.split(', ')\n",
    "    for i, word in enumerate(words):\n",
    "        if '2' in word:\n",
    "            words[i] = word.replace('2', '') + ', ' + word.replace('2', '')\n",
    "    return ', '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2가 붙은 단어를 두 번 반복한 dataframe df_dup 생성\n",
    "df_dup = df_notnull.copy()\n",
    "for column in columns:\n",
    "    df_dup[columns] = df_notnull[columns].map(duplicate_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_dup.columns'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_dup.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옷의 조합 컬럼 생성 (상의, 아우터, 하의, 신발, 엑세서리의 각 값들을 하나의 문자열로 조합하여 하나의 컬럼으로 만듦)\n",
    "df_combination = df_dup.copy()\n",
    "df_combination['옷 조합'] = df_dup['상의'] + ', ' + df_dup['아우터'] + ', ' + df_dup['하의'] + ', ' + df_dup['신발'] + ', ' + df_dup['액세서리']\n",
    "df_combination.drop(columns=['상의', '아우터', '하의', '신발', '액세서리'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_combination.columns'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_combination.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_combination[df_combination['옷 조합'].str.contains('니트 , 니트')]\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_combination[df_combination['옷 조합'].str.contains('니트 , 니트')]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# 옷의 조합 컬럼의 공백 제거\\ndf_combination['옷 조합'] = df_combination['옷 조합'].str.replace(' ', '')\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# 옷의 조합 컬럼의 공백 제거\n",
    "df_combination['옷 조합'] = df_combination['옷 조합'].str.replace(' ', '')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksy/anaconda3/envs/ondoset/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'npa.shape'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쉼표를 기준으로 텍스트를 나누는 함수\n",
    "def comma_tokenizer(s):\n",
    "    return s.split(', ')\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=comma_tokenizer)\n",
    "\n",
    "O = vectorizer.fit_transform(df_combination['옷 조합'])\n",
    "\n",
    "# multi-hot encoding된 데이터를 numpy array로 변환\n",
    "df_encoded = pd.DataFrame(O.toarray().tolist(), columns=vectorizer.get_feature_names_out())\n",
    "npa = np.array(df_encoded)\n",
    "'''npa.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([86, 317, 430, 435, 560, 593, 633, 640, 793, 1039], dtype='int64')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 값이 2 이상인 행의 인덱스\n",
    "rows_with_value_2 = df_encoded[(df_encoded >= 2).any(axis=1)]\n",
    "rows_with_value_2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'record'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 값이 2 이상인 열의 이름을 찾습니다.\n",
    "columns_with_value_over_2 = df_encoded.columns[(df_encoded >= 2).any()]\n",
    "\n",
    "# 특정 행에 대해 이를 기록합니다.\n",
    "record = df_encoded.loc[rows_with_value_2.index, columns_with_value_over_2]\n",
    "'''record'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorizer.get_feature_names_out()'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어장 확인\n",
    "'''vectorizer.get_feature_names_out()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_combination['옷 조합']\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array를 list로 변환 후 clothes_combination 컬럼에 대입\n",
    "df_combination['옷 조합'] = npa.tolist()\n",
    "'''df_combination['옷 조합']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_combination['옷 조합'] \""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi-hot encoding된 데이터를 다시 텍스트로 변환\n",
    "df_combination['옷 조합'] = vectorizer.inverse_transform(npa)\n",
    "'''df_combination['옷 조합'] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문자열로 변환\n",
    "df_combtest = df_combination.copy()\n",
    "df_combtest['옷 조합'] = df_combination['옷 조합'].apply(lambda x: ', '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-hot encoding의 값이 2 이상인 경우, 해당 단어를 두 번 반복\n",
    "for i in record.index:\n",
    "    old_value = df_combtest.loc[i, '옷 조합']\n",
    "    for col in record.columns:\n",
    "        if record.loc[i, col] >= 2:\n",
    "            old_value = old_value.replace(col, col + ', ' + col)\n",
    "    df_combtest.loc[i, '옷 조합'] = old_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_combtest.loc[rows_with_value_2.index, '옷 조합']\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_combtest.loc[rows_with_value_2.index, '옷 조합']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_combtest'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_combtest'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(max_temp, min_temp)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평균기온(°C) column의 최대값과 최솟값\n",
    "max_temp = df_combtest['평균기온(°C)'].max()\n",
    "min_temp = df_combtest['평균기온(°C)'].min()\n",
    "'''print(max_temp, min_temp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_limit'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_limit = df_combtest.copy()\n",
    "# 평균기온(°C) column을 5도 간격으로 범주화하여 0, 1, 2, ...로 변환\n",
    "bins=np.round(np.arange(min_temp -5, max_temp+5, 5), 1)\n",
    "labels=np.arange(0, (max_temp-min_temp)//5+2)\n",
    "df_limit['평균기온(°C)'] = pd.cut(df_limit['평균기온(°C)'], bins=bins, labels=labels)\n",
    "'''df_limit'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.0, 1.0, 2.0, 3.0, 7.0, 8.0, 9.0], 2: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 9.0], 3: [0.0, 1.0, 2.0, 5.0, 9.0], 4: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 5: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 6: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 7: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 8: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0], 9: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 10: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 11: [0.0, 1.0, 2.0, 3.0, 4.0, 9.0], 12: [0.0, 1.0, 2.0, 5.0, 6.0, 7.0, 8.0, 9.0], 13: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 14: [0.0, 1.0, 2.0, 9.0]}\n"
     ]
    }
   ],
   "source": [
    "# '평균기온(°C)'의 각 범주를 고려하여 데이터를 분할\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "# 각 user별로 온도 범주의 데이터가 적은 경우 기록\n",
    "user_category_not_valid = {}\n",
    "\n",
    "for user in df_limit['userId'].unique():\n",
    "    for category in labels:\n",
    "        category_data = df_limit[(df_limit['평균기온(°C)'] == category) & (df_limit['userId'] == user)]\n",
    "        \n",
    "        if category_data.shape[0] < 20:\n",
    "            if user not in user_category_not_valid:\n",
    "                user_category_not_valid[user] = [category]\n",
    "            else:\n",
    "                user_category_not_valid[user].append(category)\n",
    "            train_data.append(category_data)\n",
    "            continue\n",
    "\n",
    "        # 먼저 전체 데이터의 50%를 훈련 데이터로 분할\n",
    "        train, temp = train_test_split(category_data, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # 남은 데이터를 반으로 나누어 검증 데이터와 테스트 데이터로 분할\n",
    "        val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "        \n",
    "        train_data.append(train)\n",
    "        val_data.append(val)\n",
    "        test_data.append(test)\n",
    "\n",
    "print(user_category_not_valid)\n",
    "# 각 데이터 세트를 하나의 DataFrame으로 병합\n",
    "train_data_df = pd.concat(train_data)\n",
    "val_data_df = pd.concat(val_data)\n",
    "test_data_df = pd.concat(test_data)\n",
    "\n",
    "# 평균기온 column을 범주형으로 변경\n",
    "train_data_df['평균기온(°C)'] = train_data_df['평균기온(°C)'].astype('float64')\n",
    "val_data_df['평균기온(°C)'] = val_data_df['평균기온(°C)'].astype('float64')\n",
    "test_data_df['평균기온(°C)'] = test_data_df['평균기온(°C)'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table을 이용한 user-item matrix 생성\n",
    "train_data_df_value = train_data_df.copy()\n",
    "train_data_df_value['평균기온(°C)'] = train_data_df_value['평균기온(°C)'].astype('float32')\n",
    "UI_temp = train_data_df_value.pivot_table(index='userId', columns='옷 조합', values='평균기온(°C)', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI_val = UI_temp.copy()\n",
    "# UI_val의 값을 모두 0으로 초기화\n",
    "UI_val = UI_val.map(lambda x: 0.0)\n",
    "for user in UI_temp.index:\n",
    "    for item in UI_temp.columns:\n",
    "        # validation에 해당 user-item이 있는 경우 해당 user-item의 평균을 기록\n",
    "        if item in val_data_df[val_data_df['userId'] == user]['옷 조합'].values:\n",
    "            UI_val.loc[user, item] = val_data_df[(val_data_df['userId'] == user) & (val_data_df['옷 조합'] == item)]['평균기온(°C)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI_test = UI_temp.copy()\n",
    "# UI_test의 값을 모두 0으로 초기화\n",
    "UI_test = UI_test.map(lambda x: 0.0)\n",
    "for user in UI_temp.index:\n",
    "    for item in UI_temp.columns:\n",
    "        # test에 해당 user-item이 있는 경우 해당 user-item의 평균을 기록\n",
    "        if item in test_data_df[test_data_df['userId'] == user]['옷 조합'].values:\n",
    "            UI_test.loc[user, item] = test_data_df[(test_data_df['userId'] == user) & (test_data_df['옷 조합'] == item)]['평균기온(°C)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI_val[UI_val != 0].count().sum(), UI_test[UI_test != 0].count().sum()'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI_val, UI_test의 값이 0이 아닌 경우의 개수\n",
    "'''UI_val[UI_val != 0].count().sum(), UI_test[UI_test != 0].count().sum()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table을 이용한 user_\n",
    "UI_count = train_data_df.pivot_table( index='userId', columns='옷 조합', aggfunc='size', fill_value=0)\n",
    "# 해당 user의 총 예제 개수로 각각의 row를 나눔\n",
    "UI_count_div = UI_count.div(UI_count.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 9, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1,\n",
       "       3, 1, 1, 2, 1, 1, 1, 2, 3, 4, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 2, 2, 1, 1, 1, 3, 3, 5, 1, 5, 1, 2, 1, 1, 2, 3, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 5, 3, 1, 2, 2, 1,\n",
       "       1, 1, 2, 4, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 6, 3, 3, 2, 1, 1, 2, 1, 5,\n",
       "       1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3,\n",
       "       1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 5, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 5, 1, 4, 3, 1, 4, 1, 5, 1, 2, 1, 1, 1, 1, 1, 4, 2,\n",
       "       4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1,\n",
       "       4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 3, 1,\n",
       "       1, 1, 4, 5, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 3,\n",
       "       2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user-item matrix에 기록된 값이 존재하는 경우 1, 아닌 경우 0으로 변환하여 R_df에 기록\n",
    "R_df = UI_temp.map(lambda x: 1 if x != 0 else 0)\n",
    "R_np = np.array(R_df)\n",
    "R_np.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tensor에 기록된 값이 존재하는 경우 1, 존재하지 않는 경우 0\\nR_np = np.where(tensor > 0, 1, 0)\\nsum_along_all_other_axes = R_np.sum(axis=tuple(range(1, R_np.ndim)))\\nsum_along_all_other_axes'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# tensor에 기록된 값이 존재하는 경우 1, 존재하지 않는 경우 0\n",
    "R_np = np.where(tensor > 0, 1, 0)\n",
    "sum_along_all_other_axes = R_np.sum(axis=tuple(range(1, R_np.ndim)))\n",
    "sum_along_all_other_axes'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열의 합이 2 이상(여러 유저가 해당 옷 조합을 선택한 경우)인 열을 찾음\n",
    "columns_with_sum_over_2 = R_df.columns[R_df.sum() >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UI_temp[columns_with_sum_over_2]'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''UI_temp[columns_with_sum_over_2]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'columns_with_sum_over_2'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''columns_with_sum_over_2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 해당 조합의 인덱스\\ncolumn_index = []\\nfor i in columns_with_sum_over_2:\\n    column_index.append(R_df.columns.get_loc(i))\\n    column_index'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# 해당 조합의 인덱스\n",
    "column_index = []\n",
    "for i in columns_with_sum_over_2:\n",
    "    column_index.append(R_df.columns.get_loc(i))\n",
    "    column_index'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for row in R_np:\\n    print(row)'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for row in R_np:\n",
    "    print(row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570, 14)\n"
     ]
    }
   ],
   "source": [
    "# CF를 위한 초기값 설정\n",
    "Y = np.array(UI_temp) \n",
    "Y = Y.T\n",
    "count = np.array(UI_count_div)\n",
    "count = count.T\n",
    "print(Y.shape)\n",
    "R = Y != 0 \n",
    "n_u = Y.shape[1]\n",
    "n_o = Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation, test\n",
    "Y_val = np.array(UI_val)\n",
    "Y_val = Y_val.T\n",
    "Y_test = np.array(UI_test)\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기록이 존재하는 값의 평균을 구함\n",
    "o_sum = Y.sum(axis=1)\n",
    "o_count = R.sum(axis=1)\n",
    "o_mean = o_sum / o_count\n",
    "o_mean = o_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y[column_index]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Y[column_index]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_stand = Y - (o_mean * R)\n",
    "Y_val_stand = Y_val - (o_mean * (Y_val != 0))\n",
    "Y_test_stand = Y_test - (o_mean * (Y_test != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_v(O, U, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(O, tf.transpose(U)) + b - Y )*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(O**2) + tf.reduce_sum(U**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user, outfit의 수\n",
    "n_o, n_u = Y.shape\n",
    "\n",
    "\n",
    "# (U,O)를 초기화하고 tf.Variable로 등록하여 추적\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "U = tf.Variable(tf.random.normal((n_u,  num_features),dtype=tf.float64),  name='U')\n",
    "O = tf.Variable(tf.random.normal((n_o, num_features),dtype=tf.float64),  name='O')\n",
    "b = tf.Variable(tf.random.normal((1,          n_u),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# optimizer 초기화\n",
    "optimizer = keras.optimizers.Adam(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = cofi_cost_func_v(O, U, b, Y_stand, R, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (with regularization): 27376.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost (with regularization): {J:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(O, U, b, o_mean, count, count_weight, df, UI_temp, labels, user_category_not_valid, isTrain=False) :\n",
    "    # 예측을 수행하기 위해 모든 user-item에 대한 예측값을 계산\n",
    "    p = np.matmul(O.numpy(), np.transpose(U.numpy())) + b.numpy()\n",
    "    # user_category_not_valid에 해당하지 않는 경우에 대해 precision, recall, f1_score 계산\n",
    "    # 평균을 위한 초기화\n",
    "    precision_m, recall_m, f1_score_m, count_m = 0, 0, 0, 0\n",
    "    for i in range(UI_temp.shape[0]):\n",
    "        for category in labels:\n",
    "            \n",
    "            # 실제 온도\n",
    "            # 평균을 적용하고 temp를 빼서 값이 작을수록 실제 온도에 가깝도록 함. 이 때 각 user-item의 사용 횟수를 가중하여 많이 사용한 item이 추천되도록 함\n",
    "            pm = np.power(p + o_mean - category, 2)  -count * count_weight\n",
    "            my_predictions = pm[:,i]\n",
    "\n",
    "            # sort predictions\n",
    "            ix = tf.argsort(my_predictions, direction='ASCENDING')\n",
    "\n",
    "            df_predict = UI_temp[UI_temp.columns[ix[0:3]]].copy()\n",
    "            df_predict = df_predict.round(0)\n",
    "            # df_predict의 columns와 test_data_df의 '옷 조합' column을 비교하여 일치하는 경우의 개수를 계산\n",
    "            predict = df_predict.columns.astype(str)\n",
    "            \n",
    "            if not isTrain:\n",
    "                # user i에 대한 예측을 파일로 저장\n",
    "                os.makedirs(f'../data/predictions/male/user_{i+1}', exist_ok=True)\n",
    "                # Save predictions to file in user's directory\n",
    "                with open(f'../data/predictions/male/user_{i+1}/predictions_{category}.txt', 'w') as f:\n",
    "                    for item in predict:\n",
    "                        f.write(\"%s\\n\" % item)\n",
    "            \n",
    "            if i+1 in user_category_not_valid and category in user_category_not_valid[i+1]:\n",
    "                '''print(f'{i+1}번 user, {category}도 데이터가 부족하여 제외합니다.')'''\n",
    "                continue\n",
    "            \n",
    "            label = df[(df['userId'] == i+1) & (df['평균기온(°C)'] == category)]['옷 조합'].astype(str)\n",
    "            # label이 UI_temp의 column에 포함되지 않는다면 제외\n",
    "            label = label[label.isin(UI_temp.columns)]\n",
    "            # label에 어떠한 옷 조합도 포함되지 않을 시 지표를 측정하지 않음\n",
    "            if label.shape[0] == 0:\n",
    "                '''print(f'{i+1}번 user, {category}도 label 데이터가 부족하여 제외합니다.')'''\n",
    "                continue\n",
    "            \n",
    "            count_m += 1\n",
    "            precision = len(set(predict) & set(label)) / len(set(predict))\n",
    "            '''print(f'{i+1}번 user, {category}도 prediction: {predict}')\n",
    "            print(f'{i+1}번 user, {category}도 label (개수: {len(set(label))}): {label}')'''\n",
    "            recall = len(set(predict) & set(label)) / len(set(label))\n",
    "            if precision + recall == 0:\n",
    "                '''print(f'0인 경우')\n",
    "                print(f'{i+1}번 user, {category}도 예측 결과: {predict}, 실제 결과: {label} ')'''  \n",
    "                f1_score = 0\n",
    "            else:\n",
    "                '''print(f'0이 아닌 경우')\n",
    "                print(f'{i+1}번 user, {category}도 예측 결과: {predict}, 실제 결과: {label} ')'''  \n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "            precision_m += precision\n",
    "            recall_m += recall\n",
    "            f1_score_m += f1_score\n",
    "            '''print(precision, recall, f1_score)'''\n",
    "    precision_m /= count_m\n",
    "    recall_m /= count_m\n",
    "    f1_score_m /= count_m\n",
    "    return precision_m, recall_m, f1_score_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for iter in range(iterations):\n",
    "    # TensorFlow의 GradientTape 사용\n",
    "    # 연산을 기록하여 cost에 대한 gradient를 자동으로 계산\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # cost 계산 (forward pass included in cost)\n",
    "        cost_value = cofi_cost_func_v(O, U, b, Y_stand, R, lambda_)\n",
    "\n",
    "    # GradientTape를 통해 자동 미분\n",
    "    # loss에 대한 trainable parameter의 gradient를 계산\n",
    "    grads = tape.gradient( cost_value, [O,U,b] )\n",
    "\n",
    "    # optimizer를 사용하여 trainable parameter를 업데이트\n",
    "    optimizer.apply_gradients( zip(grads, [O,U,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if (iter + 1) % 20 == 0 or iter == 0:\n",
    "        train_loss = cost_value.numpy()\n",
    "        history.append({'type': 'train_loss', 'epoch': iter + 1, 'value': train_loss})\n",
    "\n",
    "        val_loss = cofi_cost_func_v(O, U, b, Y_val_stand, R, lambda_).numpy()\n",
    "        history.append({'type': 'validation_loss', 'epoch': iter + 1, 'value': val_loss})\n",
    "\n",
    "        precision, recall, f1_score = metrics(O, U, b, o_mean, count, count_weight, val_data_df, UI_temp, labels, user_category_not_valid, isTrain=True)\n",
    "        history.append({'type': 'validation precision', 'epoch': iter + 1, 'value': precision})\n",
    "        history.append({'type': 'validation recall', 'epoch': iter + 1, 'value': recall})\n",
    "        history.append({'type': 'validataion f1_score', 'epoch': iter + 1, 'value': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"train_result\" : history,\n",
    "    \"data_count\" : df_combtest.shape[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train_result\": [{\"type\": \"train_loss\", \"epoch\": 1, \"value\": 22939.909720350537}, {\"type\": \"validation_loss\", \"epoch\": 1, \"value\": 14697.358373507495}, {\"type\": \"validation precision\", \"epoch\": 1, \"value\": 0.07575757575757575}, {\"type\": \"validation recall\", \"epoch\": 1, \"value\": 0.043109668109668105}, {\"type\": \"validataion f1_score\", \"epoch\": 1, \"value\": 0.052380952380952375}, {\"type\": \"train_loss\", \"epoch\": 20, \"value\": 1901.2386086494148}, {\"type\": \"validation_loss\", \"epoch\": 20, \"value\": 1909.2738393754723}, {\"type\": \"validation precision\", \"epoch\": 20, \"value\": 0.18181818181818182}, {\"type\": \"validation recall\", \"epoch\": 20, \"value\": 0.0910894660894661}, {\"type\": \"validataion f1_score\", \"epoch\": 20, \"value\": 0.11875901875901879}, {\"type\": \"train_loss\", \"epoch\": 40, \"value\": 381.4826159030821}, {\"type\": \"validation_loss\", \"epoch\": 40, \"value\": 512.3783109013893}, {\"type\": \"validation precision\", \"epoch\": 40, \"value\": 0.21212121212121213}, {\"type\": \"validation recall\", \"epoch\": 40, \"value\": 0.13300865800865802}, {\"type\": \"validataion f1_score\", \"epoch\": 40, \"value\": 0.1566017316017316}, {\"type\": \"train_loss\", \"epoch\": 60, \"value\": 121.25420541608142}, {\"type\": \"validation_loss\", \"epoch\": 60, \"value\": 272.7557491447387}, {\"type\": \"validation precision\", \"epoch\": 60, \"value\": 0.18181818181818182}, {\"type\": \"validation recall\", \"epoch\": 60, \"value\": 0.11255411255411256}, {\"type\": \"validataion f1_score\", \"epoch\": 60, \"value\": 0.13225108225108226}, {\"type\": \"train_loss\", \"epoch\": 80, \"value\": 71.74421174144096}, {\"type\": \"validation_loss\", \"epoch\": 80, \"value\": 230.76125099848807}, {\"type\": \"validation precision\", \"epoch\": 80, \"value\": 0.21212121212121213}, {\"type\": \"validation recall\", \"epoch\": 80, \"value\": 0.14437229437229437}, {\"type\": \"validataion f1_score\", \"epoch\": 80, \"value\": 0.1617965367965368}, {\"type\": \"train_loss\", \"epoch\": 100, \"value\": 60.63726631613992}, {\"type\": \"validation_loss\", \"epoch\": 100, \"value\": 219.91015549105634}, {\"type\": \"validation precision\", \"epoch\": 100, \"value\": 0.18181818181818182}, {\"type\": \"validation recall\", \"epoch\": 100, \"value\": 0.11255411255411256}, {\"type\": \"validataion f1_score\", \"epoch\": 100, \"value\": 0.13225108225108226}, {\"type\": \"train_loss\", \"epoch\": 120, \"value\": 57.225198932356065}, {\"type\": \"validation_loss\", \"epoch\": 120, \"value\": 216.1989092260295}, {\"type\": \"validation precision\", \"epoch\": 120, \"value\": 0.196969696969697}, {\"type\": \"validation recall\", \"epoch\": 120, \"value\": 0.12391774891774893}, {\"type\": \"validataion f1_score\", \"epoch\": 120, \"value\": 0.14523809523809525}, {\"type\": \"train_loss\", \"epoch\": 140, \"value\": 55.811232238077544}, {\"type\": \"validation_loss\", \"epoch\": 140, \"value\": 214.2013488215017}, {\"type\": \"validation precision\", \"epoch\": 140, \"value\": 0.196969696969697}, {\"type\": \"validation recall\", \"epoch\": 140, \"value\": 0.13528138528138528}, {\"type\": \"validataion f1_score\", \"epoch\": 140, \"value\": 0.15043290043290042}], \"data_count\": 1338}\n"
     ]
    }
   ],
   "source": [
    "json_data = json.dumps(data)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables_optimizer(variables, optimizer, filename):\n",
    "    checkpoint = tf.train.Checkpoint(variables=variables, optimizer=optimizer)\n",
    "    checkpoint.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 tf.Variable 파일로 저장\n",
    "model_version = '1.0'\n",
    "checkpoint_path = f'../model/CF/train/{model_version}/'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "save_variables_optimizer({\"O\": O, \"U\": U, \"b\": b}, optimizer,  checkpoint_path+\"parameters.ckpt\")\n",
    "\n",
    "# UI_temp및 UI_count_div를 저장\n",
    "UI_temp.to_csv(checkpoint_path + 'UI_temp.csv')\n",
    "UI_count_div.to_csv(checkpoint_path + 'UI_count_div.csv')\n",
    "\n",
    "# loss 측정을 위한 test 데이터 저장\n",
    "test_for_loss = pd.DataFrame(Y_test_stand)\n",
    "test_for_loss.to_csv(checkpoint_path + 'test_for_loss.csv')\n",
    "\n",
    "# metric을 계산하기 위한 데이터 저장\n",
    "test_data_df.to_csv(checkpoint_path + 'test_data_df.csv')\n",
    "\n",
    "# user_category_not_valid 및 unique category 저장\n",
    "user_category_not_valid_df = pd.DataFrame(user_category_not_valid.items(), columns=['userId', 'category'])\n",
    "user_category_not_valid_df.to_csv(checkpoint_path + 'user_category_not_valid.csv')\n",
    "category = df_limit['평균기온(°C)'].unique()\n",
    "\n",
    "# 범주 및 해당 범주의 온도 범위를 확인하여 category_df의 columns에 저장\n",
    "categorized_temperature = pd.cut([-1.8], bins=bins, labels=labels)\n",
    "l_temp = min_temp -5\n",
    "category_values = []\n",
    "for i in range(len(bins) - 1):\n",
    "    category_values.append(f'({bins[i]}, {bins[i+1]}]')\n",
    "category_df = pd.DataFrame(columns=categorized_temperature.categories, data=[category_values])\n",
    "category_df.to_csv(checkpoint_path + 'category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0]\n",
       "Categories (10, float64): [0.0 < 1.0 < 2.0 < 3.0 ... 6.0 < 7.0 < 8.0 < 9.0]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-16.8, -11.8,  -6.8,  -1.8,   3.2,   8.2,  13.2,  18.2,  23.2,\n",
       "        28.2,  33.2])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5.0\n",
       "1       6.0\n",
       "2       6.0\n",
       "3       6.0\n",
       "4       6.0\n",
       "       ... \n",
       "1333    6.0\n",
       "1334    5.0\n",
       "1335    6.0\n",
       "1336    6.0\n",
       "1337    6.0\n",
       "Name: 평균기온(°C), Length: 1338, dtype: category\n",
       "Categories (10, float64): [0.0 < 1.0 < 2.0 < 3.0 ... 6.0 < 7.0 < 8.0 < 9.0]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_limit['평균기온(°C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'real_temperature'는 실제 온도입니다.\n",
    "real_temperature = -6.8\n",
    "\n",
    "# 실제 온도를 범주화합니다.\n",
    "categorized_temperature = pd.cut([real_temperature], bins=np.round(np.arange(min_temp -5, max_temp+5, 5), 1), labels=np.arange(0, (max_temp-min_temp)//5+2))\n",
    "\n",
    "# 범주 및 해당 범주의 온도 범위를 출력합니다\n",
    "l_temp = min_temp -5\n",
    "\n",
    "\n",
    "# 첫 번째 요소를 선택하여 범주화된 온도를 얻습니다.\n",
    "categorized_temperature = categorized_temperature[0]\n",
    "categorized_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 precision: 0.21739130434782608, 평균 recall: 0.11633885438233263, 평균 f1_score: 0.14745592571679528\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score = metrics(O, U, b, o_mean, count, count_weight, test_data_df, UI_temp, labels, user_category_not_valid, isTrain=False)\n",
    "print(f'평균 precision: {precision}, 평균 recall: {recall}, 평균 f1_score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=55.76369251458083>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cofi_cost_func_v(O, U, b, Y_stand, R, lambda_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ondoset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
